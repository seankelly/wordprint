#!/usr/bin/env python
"""
Create word fingerprints from CSV dumps of the comments table of a
Wordpress site.
"""

import csv
import re
import sys
from collections import defaultdict
from HTMLParser import HTMLParser

valid_words = set()

def usage():
    print "usage: %s CSV ..." % (sys.argv[0])
    sys.exit(0)

def load_dictionary():
    global valid_words
    with open('/usr/share/dict/words', 'rb') as f:
        for word in f:
            w = word.strip().lower()
            valid_words.add(w)
    valid_words.remove('twitter')

class HTMLStrip(HTMLParser):
    def __init__(self):
        HTMLParser.__init__(self)
        self.fed = []

    def handle_data(self, data):
        self.fed.append(data)

    def stripped_html(self):
        return ''.join(self.fed)

def read_comments(csv_file):
    # Field 3: user
    # Field 9: text
    f = open(csv_file, 'rb')
    c = csv.reader(f, delimiter=',', doublequote=False, escapechar='\\')
    for l in c:
        yield l[2], l[8]

def generate_wordprint(comments_csv):
    # User dict to map user => word => count.
    user_wordprint = defaultdict(
        lambda: defaultdict(
            lambda: 0))
    # Track how often each word is used.
    word_usage = defaultdict(lambda: 0)
    find_words = re.compile('(\w+)')
    global valid_words
    for user, text in read_comments(comments_csv):
        s = HTMLStrip()
        # Convert to lowercase.
        s.feed(unicode(text, encoding='utf8'))
        lower_text = s.stripped_html().lower()
        # Grab all 'words'. Using the regex understanding of what a word is.
        words = find_words.findall(lower_text)
        for w in words:
            if w not in valid_words:
                continue
            if len(w) <= 3:
                continue
            word_usage[w] += 1
            user_wordprint[user][w] += 1
    summarize_wordprint(user_wordprint, word_usage)

def summarize_wordprint(user_wordprint, word_usage):
    def sort_words(user):
        # Given a user, return a lambda to sort the unique words in descending
        # order.
        u = user_wordprint[user]
        return lambda a,b: cmp(u[b], u[a])

    top_words = sorted(word_usage,
        cmp=lambda a,b: cmp(word_usage[b], word_usage[a]))
    popular_words = set(top_words[0:250])
    for user in user_wordprint:
        user_words = set(user_wordprint[user].keys())
        # If fewer than 100 unique words have been uttered by this person, skip
        # them.
        if len(user_words) < 100:
            continue
        user_unique = user_words - popular_words
        user_print = sorted(user_unique, sort_words(user))
        # If no unique words, skip this user.
        if len(user_print) == 0:
            continue
        top_five = map(lambda x: "%s" % (x),
                user_print[0:5])
        print "%s: %s" % (user, ', '.join(top_five))

if __name__ == '__main__':
    if len(sys.argv) == 1:
        usage()

    load_dictionary()
    for comments_csv in sys.argv[1:]:
        generate_wordprint(comments_csv)
