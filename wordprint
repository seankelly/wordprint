#!/usr/bin/env python3
"""
Create word fingerprints from CSV dumps of the comments table of a
Wordpress site.
"""

import argparse
import csv
import logging
import re
import sys
from collections import defaultdict
from html.parser import HTMLParser

valid_words = set()

# Number of words to include in each user's fingerprint.
FINGERPRINT_WORDS = 5
# Minimum number of comments by user to have a wordprint generated.
MINIMUM_COMMENTS = 100
# Number of total users that can shared a given word.
SHARED_USERS = 1
POPULAR_WORDS = 250
MAX_ITERATIONS = 50

LOG = logging.getLogger()


def load_dictionary():
    global valid_words
    with open('/usr/share/dict/words', 'r') as f:
        for word in f:
            w = word.strip().lower()
            valid_words.add(w)
    valid_words.remove('twitter')


class HTMLStrip(HTMLParser):
    def __init__(self):
        HTMLParser.__init__(self)
        self.fed = []

    def handle_data(self, data):
        self.fed.append(data)

    def stripped_html(self):
        return ''.join(self.fed)


class Wordprint:

    DEFAULT_DICTIONARY = '/usr/share/dict/words'

    def __init__(self, fingerprint_words=None, minimum_comments=None, shared_users=None):
        self.valid_words = set()
        self.fingerprint_words = fingerprint_words or FINGERPRINT_WORDS
        self.shared_users = shared_users or SHARED_USERS
        self.minimum_comments = minimum_comments or MINIMUM_COMMENTS
        # Number of comments seen by that user.
        self._user_comments = defaultdict(lambda: 0)
        # User dict to map user => word => count.
        self._user_wordprint = defaultdict(
            lambda: defaultdict(
                lambda: 0))
        # Track how often each word is used.
        self._word_usage = defaultdict(lambda: 0)

    def load_dictionary(self, dictionary_file=None):
        valid_words = set()
        if not dictionary_file:
            dictionary_file = self.DEFAULT_DICTIONARY
        with open(dictionary_file, 'r') as dictionary:
            for word in dictionary:
                lowercase_word = word.strip().lower()
                valid_words.add(lowercase_word)
        valid_words.remove('twitter')
        self.valid_words.update(valid_words)
        LOG.info("Total word count: %d", len(self.valid_words))

    def load_comments(self, csv_file):
        if not self.valid_words:
            self.load_dictionary()
        find_words = re.compile("([\w-]+(?:'[\w+-]+)?)")
        with open(csv_file, 'r') as comments_input:
            comments = csv.reader(comments_input, delimiter=',', doublequote=False,
                                  escapechar='\\')
            for line in comments:
                user, text = line[2], line[8]
                s = HTMLStrip()
                # Convert to lowercase.
                s.feed(text)
                lower_text = s.stripped_html().lower()
                # Grab all 'words'. Using the regex understanding of what a word is.
                words = find_words.findall(lower_text)
                for word in words:
                    if word not in self.valid_words:
                        continue
                    if len(word) <= 3:
                        continue
                    self._word_usage[word] += 1
                    self._user_wordprint[user][word] += 1
                self._user_comments[user] += 1
        LOG.info("Total user count: %d", len(self._user_wordprint))

    def generate_wordprint(self):
        user_sorted_words = self._summarize_wordprint()
        user_top_words, user_word_idx = self._find_unique_words(user_sorted_words)
        for user in sorted(user_top_words, key=str.lower):
            word_print = ', '.join(sorted(user_top_words[user]))
            print(f"{user} ({user_word_idx[user]}): {word_print}")

    def _find_unique_words(self, user_sorted_words):
        user_top_words = {}
        user_word_idx = {}
        # Map word => set of users with that word.
        word_usage = defaultdict(set)
        # Get the top N words for each user.
        for user in user_sorted_words:
            top_words = set()
            # Pop off the words because it's sorted from least to most common.
            # It will be easy to take more when needed later.
            for _ in range(self.fingerprint_words):
                word = user_sorted_words[user].pop()
                top_words.add(word)
                word_usage[word].add(user)
            user_top_words[user] = top_words
            user_word_idx[user] = self.fingerprint_words

        update_word_print_users = set()
        # Count the number of iterations to prevent it from blowing up.
        iterations = 1
        # Scan all users every time to keep the wordprint stable.
        word_collisions = True
        while word_collisions and iterations <= MAX_ITERATIONS:
            # Assume no collisions and then try to find one.
            word_collisions = False
            print(f"Iteration {iterations}", flush=True)
            word_list = list(word_usage.items())
            for word, users in word_list:
                # Too many users with that word. No one gets it!
                if len(users) > SHARED_USERS:
                    LOG.debug("Rejecting %s", word)
                    word_collisions = True
                    for u in users:
                        user_top_words[u].remove(word)
                        update_word_print_users.add(u)
                    del word_usage[word]
            # For each user that was modified somehow, add in more words for them
            # to bring their total back up to the required number, if possible.
            for user in update_word_print_users:
                while len(user_top_words[user]) < self.fingerprint_words:
                    next_word = user_sorted_words[user].pop()
                    user_top_words[user].add(next_word)
                    word_usage[next_word].add(user)
                    user_word_idx[user] += 1
            update_word_print_users.clear()
            iterations += 1
        return user_top_words, user_word_idx

    def _summarize_wordprint(self):
        top_words = sorted(self._word_usage, key=self._word_usage.get, reverse=True)
        popular_words = set(top_words[:POPULAR_WORDS])
        user_sorted_words = {}

        for user, user_word_usage in self._user_wordprint.items():
            if self._user_comments[user] < self.minimum_comments:
                continue
            user_words = set(user_word_usage)
            # If fewer than 100 unique words have been uttered by this person, skip
            # them.
            if len(user_words) < 100:
                continue
            unique_words = user_words - popular_words
            # If no words outside of the popular words, skip this user.
            if not unique_words:
                continue
            # Sort so the most used words are at the end. This will make
            # getting the next most common word an easier operation.
            user_print = sorted(unique_words, key=user_word_usage.get)
            user_sorted_words[user] = user_print
            LOG.debug("unique words for %s: %s", user, user_print)
        return user_sorted_words


def read_comments(csv_file):
    # Field 3: user
    # Field 9: text
    f = open(csv_file, 'r')
    c = csv.reader(f, delimiter=',', doublequote=False, escapechar='\\')
    for l in c:
        yield l[2], l[8]


def generate_wordprint(comments_csv):
    # User dict to map user => word => count.
    user_wordprint = defaultdict(
        lambda: defaultdict(
            lambda: 0))
    # Track how often each word is used.
    word_usage = defaultdict(lambda: 0)
    find_words = re.compile("([\w-]+(?:'[\w+-]+)?)")
    global valid_words
    for user, text in read_comments(comments_csv):
        s = HTMLStrip()
        # Convert to lowercase.
        s.feed(text)
        lower_text = s.stripped_html().lower()
        # Grab all 'words'. Using the regex understanding of what a word is.
        words = find_words.findall(lower_text)
        for w in words:
            if w not in valid_words:
                continue
            if len(w) <= 3:
                continue
            word_usage[w] += 1
            user_wordprint[user][w] += 1
    summarize_wordprint(user_wordprint, word_usage)


def find_unique_words(user_sorted_words):
    user_top_words = {}
    user_word_idx = {}
    # Get the top N words for each user.
    for user in user_sorted_words:
        top_words = user_sorted_words[user][0:FINGERPRINT_WORDS]
        user_top_words[user] = set(top_words)
        user_word_idx[user] = FINGERPRINT_WORDS

    # check_users is the set of users to check. Initialize it with all users.
    # For each user, check if any other users share words. If greater than
    # SHARED_USERS, remove that word from each user's set.
    check_users = set(user_sorted_words)
    next_check_users = set()
    # Count the number of iterations to prevent it from blowing up.
    iterations = 0
    while check_users and iterations < 50:
        for user in check_users:
            # Force a copy of the set. The set may be modified during
            # iteration.
            words = list(user_top_words[user])
            for word in words:
                matching_users = set()
                for compare_user in user_top_words:
                    if word in user_top_words[compare_user]:
                        matching_users.add(compare_user)
                if len(matching_users) > SHARED_USERS:
                    # Too many users with that word. No one gets it!
                    for u in matching_users:
                        user_top_words[u].remove(word)
                        next_check_users.add(u)
        # For each user that was modified somehow, add in more words for them
        # to bring their total back up to the required number, if possible.
        for user in next_check_users:
            start_idx = user_word_idx[user]
            end_idx = start_idx + FINGERPRINT_WORDS - len(user_top_words[user])
            for w in user_sorted_words[user][start_idx:end_idx]:
                user_top_words[user].add(w)
            user_word_idx[user] = end_idx
        # Update the set to check and clear the potential future set.
        check_users = next_check_users
        next_check_users = set()
        iterations += 1
        print(iterations, flush=True)
    return user_top_words, user_word_idx


def summarize_wordprint(user_wordprint, word_usage):
    top_words = sorted(word_usage, key=lambda user: word_usage[user], reverse=True)
    popular_words = set(top_words[:POPULAR_WORDS])
    user_sorted_words = {}

    for user in user_wordprint:
        user_words = set(user_wordprint[user])
        # If fewer than 100 unique words have been uttered by this person, skip
        # them.
        if len(user_words) < 100:
            continue
        user_unique = user_words - popular_words
        user_print = sorted(user_unique, key=lambda word: user_wordprint[user][word],
                            reverse=True)
        # If no unique words, skip this user.
        if not user_print:
            continue
        user_sorted_words[user] = user_print
    user_top_words, user_word_idx = find_unique_words(user_sorted_words)

    for user in sorted(user_top_words, key=str.lower):
        print('%s (%d): %s' % (user, user_word_idx[user],
                ', '.join(user_top_words[user])))


def options():
    parser = argparse.ArgumentParser("Wordprint for WordPress users")
    parser.add_argument('--words', '-w', type=int, default=FINGERPRINT_WORDS,
                        help="Number of words to include in each user's fingerprint.")
    parser.add_argument('--user-overlap', '-u', type=int, default=SHARED_USERS,
                        help="Number of total users that can share a word.")
    parser.add_argument('--verbose', '-v', default=0, action='count',
                        help="Increase program verbosity.")
    parser.add_argument('comments', nargs='+',
                        help="CSV file containing WordPress comments.")
    args = parser.parse_args()
    return args


def main():
    args = options()
    if args.verbose >= 2:
        LOG.setLevel(logging.DEBUG)
    elif args.verbose >= 1:
        LOG.setLevel(logging.INFO)
    handler = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    LOG.addHandler(handler)

    wordprint = Wordprint()
    wordprint.load_dictionary()
    for comments_csv in args.comments:
        wordprint.load_comments(comments_csv)
    user_wordprint = wordprint.generate_wordprint()


if __name__ == '__main__':
    main()
